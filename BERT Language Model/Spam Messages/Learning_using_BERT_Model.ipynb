{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Learning using BERT Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc7gCJ9feSWY",
        "colab_type": "text"
      },
      "source": [
        "## BERT\n",
        "\n",
        "Bidirectional Encoder Representations from Transformers is a technique for NLP pre-training developed by Google.  It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to TF Hub as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, finetuning BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from this colab notebook.\n",
        "\n",
        "Source for Learning:\n",
        "1. [Google-Research](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=xiYrZKaHwV81)\n",
        "2. [Stack Abuse](https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHPmJVNE3vnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c95042c7-c893-44df-c478-5b5143af29ac"
      },
      "source": [
        "!pip3 install bert-for-tf2\n",
        "!pip3 install sentencepiece\n",
        "!python3 -c \"import nltk; nltk.download('punkt'); nltk.download('wordnet')\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.4)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR-NfgZAfWMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk as nlp\n",
        "import tensorflow as tf\n",
        "import bert\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input,Dense\n",
        "import tensorflow_hub as tfhub\n",
        "import tensorflow_datasets as tfds\n",
        "from datetime import datetime"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FlXYDGzfhCa",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGCbDx8_i7U5",
        "colab_type": "text"
      },
      "source": [
        "Fetching BERT Model from TensorFlow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WfxBBG4i_jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_layer = tfhub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDBO3BmKoMvP",
        "colab_type": "text"
      },
      "source": [
        "### Importing and Cleaning Spam Messages Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSMb7hKFzTYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('SPAM Text Message Data.csv')\n",
        "df[\"Category\"] = [1 if each == \"spam\" else 0 for each in df[\"Category\"]]\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    import string\n",
        "    # replacing the punctuations with no space, \n",
        "    # which in effect deletes the punctuation marks \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)\n",
        "\n",
        "def lowerwords(text):\n",
        "\ttext = re.sub(\"[^a-zA-Z]\",\" \",text) # Excluding Numbers\n",
        "\ttext = [word.lower() for word in text.split()]\n",
        "    # joining the list of words with space separator\n",
        "\treturn \" \".join(text)\n",
        "        \n",
        "df['Message'] = df['Message'].apply(remove_punctuation)\n",
        "df['Message'] = df['Message'].apply(lowerwords)\n",
        "\n",
        "description_list = []\n",
        "for description in df[\"Message\"]:\n",
        "    description = nlp.word_tokenize(description)\n",
        "    #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
        "    lemma = nlp.WordNetLemmatizer()\n",
        "    description = [lemma.lemmatize(word) for word in description]\n",
        "    description = \" \".join(description)\n",
        "    description_list.append(description) # we hide all word one section"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTfrYGxb4QBF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "995efe33-55d0-414b-e2c0-2531d8160e21"
      },
      "source": [
        "X_Messages = df['Message']\n",
        "y_Labels = df['Category']\n",
        "\n",
        "print (X_Messages.shape,y_Labels.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5572,) (5572,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctzo0PnAp5Cj",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation for BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw6wU-ps4tK7",
        "colab_type": "text"
      },
      "source": [
        "#### Creating a BERT Tokenizer\n",
        "\n",
        "We will first create an object of the FullTokenizer class from the bert.bert_tokenization module. Next, we create a BERT Embedding Layer by importing the BERT Model from tfhub.KerasLayer. The trainable parameter is set to False, which means that we will not be training the BERT Embedding. In the next line, we create a BERT Vocabulary file in the form a numpy array. We then set the text to lowercase and finally we pass our vocabulary_file and to_lower_case variables to the BertTokenizer object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgyJSbBC4xJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = tfhub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0baP8fYm59h-",
        "colab_type": "text"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzqw7zj3p9R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_masks(tokens,max_seq_length):\n",
        "    \"\"\"\n",
        "    This Function Trims/ Pads a depending on length of token\n",
        "    \"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        # Cutting Down the Excess Length\n",
        "        tokens = tokens[0:max_seq_length]\n",
        "        return [1]*len(tokens)\n",
        "    else :\n",
        "        return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \n",
        "    if len(tokens)>max_seq_length:\n",
        "        # Cutting Down the Excess Length\n",
        "        tokens = tokens[:max_seq_length]\n",
        "        segments = []\n",
        "        current_segment_id = 0\n",
        "        for token in tokens:\n",
        "            segments.append(current_segment_id)\n",
        "            if token == \"[SEP]\":\n",
        "                current_segment_id = 1\n",
        "        return segments\n",
        "    \n",
        "    else:\n",
        "        segments = []\n",
        "        current_segment_id = 0\n",
        "        for token in tokens:\n",
        "            segments.append(current_segment_id)\n",
        "            if token == \"[SEP]\":\n",
        "                current_segment_id = 1\n",
        "        return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):    \n",
        "    if len(tokens)>max_seq_length:\n",
        "        tokens = tokens[:max_seq_length]\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        return token_ids\n",
        "    else:\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "        return input_ids"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIktltvtE44k",
        "colab_type": "text"
      },
      "source": [
        "Creating Data for BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vqkFIdx6hY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CreatingData(X_Train,tokenizer,max_seq_length=128):\n",
        "    \n",
        "    X_IDs = []\n",
        "    X_Masks = []\n",
        "    X_Segments = []\n",
        "\n",
        "    for i in range(X_Train.shape[0]):\n",
        "        x = X_Train[i]\n",
        "        x = tokenizer.tokenize(x)\n",
        "        x = [\"[CLS]\"] + x + [\"[SEP]\"]\n",
        "\n",
        "        X_IDs.append(get_ids(x, tokenizer, max_seq_length))\n",
        "        X_Masks.append(get_masks(x,max_seq_length))\n",
        "        X_Segments.append(get_segments(x, max_seq_length))\n",
        "\n",
        "    return np.array(X_IDs), np.array(X_Masks), np.array(X_Segments)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGeYOJBSF2c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "990a2555-2e83-4125-ca5f-2f3b4d2df240"
      },
      "source": [
        "X_IDs, X_Masks, X_Segments = CreatingData(X_Messages,tokenizer)\n",
        "print (X_IDs.shape)\n",
        "print (X_Masks.shape)\n",
        "print (X_Segments.shape)\n",
        "print (y_Labels.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5572, 128)\n",
            "(5572, 128)\n",
            "(5572, 128)\n",
            "(5572,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh8GEduMQHWs",
        "colab_type": "text"
      },
      "source": [
        "### Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDRy3QXRbN31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Build_Model(bert_layer=bert_layer,Max_Seq_Length=128):\n",
        "    IDs = Input(shape=(Max_Seq_Length,), dtype=tf.int32)\n",
        "    Masks = Input(shape=(Max_Seq_Length,), dtype=tf.int32)\n",
        "    Segments = Input(shape=(Max_Seq_Length,), dtype=tf.int32)\n",
        "\n",
        "    Pooled_Output, Sequence_Output = bert_layer([IDs,Masks,Segments])\n",
        "\n",
        "    x = Sequence_Output[:,0,:]\n",
        "    Output = Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "    return Model(inputs=[IDs,Masks,Segments],outputs=Output)\n",
        "\n",
        "Model = Build_Model()\n",
        "Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','AUC'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkc3ZWgpoJ7V",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USi3sGG_dF5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 523
        },
        "outputId": "41c9e646-fd23-44be-8fc2-d1c9ebfb6f59"
      },
      "source": [
        "Model.fit([X_IDs, X_Masks, X_Segments],y_Labels,epochs=10,validation_split=0.1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 29s 184ms/step - loss: 0.2717 - accuracy: 0.8821 - auc: 0.9005 - val_loss: 0.1426 - val_accuracy: 0.9516 - val_auc: 0.9868\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.1372 - accuracy: 0.9585 - auc: 0.9751 - val_loss: 0.0985 - val_accuracy: 0.9731 - val_auc: 0.9925\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.1123 - accuracy: 0.9653 - auc: 0.9786 - val_loss: 0.0832 - val_accuracy: 0.9767 - val_auc: 0.9941\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.1000 - accuracy: 0.9695 - auc: 0.9816 - val_loss: 0.0747 - val_accuracy: 0.9785 - val_auc: 0.9951\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.0921 - accuracy: 0.9713 - auc: 0.9845 - val_loss: 0.0691 - val_accuracy: 0.9785 - val_auc: 0.9959\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.0861 - accuracy: 0.9735 - auc: 0.9860 - val_loss: 0.0648 - val_accuracy: 0.9767 - val_auc: 0.9965\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.0811 - accuracy: 0.9737 - auc: 0.9880 - val_loss: 0.0616 - val_accuracy: 0.9785 - val_auc: 0.9968\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.0767 - accuracy: 0.9763 - auc: 0.9889 - val_loss: 0.0604 - val_accuracy: 0.9785 - val_auc: 0.9971\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 27s 173ms/step - loss: 0.0732 - accuracy: 0.9777 - auc: 0.9901 - val_loss: 0.0573 - val_accuracy: 0.9785 - val_auc: 0.9972\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 27s 174ms/step - loss: 0.0702 - accuracy: 0.9783 - auc: 0.9913 - val_loss: 0.0554 - val_accuracy: 0.9785 - val_auc: 0.9974\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffa24fef240>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    }
  ]
}