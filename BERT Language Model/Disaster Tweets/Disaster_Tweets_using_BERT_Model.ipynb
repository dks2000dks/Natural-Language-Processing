{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disaster-Tweets using BERT Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc7gCJ9feSWY",
        "colab_type": "text"
      },
      "source": [
        "## BERT for Prediction fo Disaster Tweets\n",
        "\n",
        "Bidirectional Encoder Representations from Transformers is a technique for NLP pre-training developed by Google.  It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
        "\n",
        "Now that BERT's been added to TF Hub as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, finetuning BERT can provide both an accuracy boost and faster training time in many cases.\n",
        "\n",
        "Here, we'll train a model to predict whether an IMDB movie review is positive or negative using BERT in Tensorflow with tf hub. Some code was adapted from this colab notebook.\n",
        "\n",
        "Source for Learning:\n",
        "1. [Google-Research](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=xiYrZKaHwV81)\n",
        "2. [Stack Abuse](https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHPmJVNE3vnu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "97f8be80-bdff-461c-fe00-318ec6bf3a2e"
      },
      "source": [
        "!pip3 install bert-for-tf2\n",
        "!pip3 install sentencepiece\n",
        "!python3 -c \"import nltk; nltk.download('punkt'); nltk.download('wordnet'); nltk.download('stopwords')\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: bert-for-tf2 in /usr/local/lib/python3.6/dist-packages (0.14.4)\n",
            "Requirement already satisfied: params-flow>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.8.2)\n",
            "Requirement already satisfied: py-params>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from bert-for-tf2) (0.9.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.91)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR-NfgZAfWMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk as nlp\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import bert\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input,Dense,LSTM,Dropout\n",
        "import tensorflow_hub as tfhub\n",
        "import tensorflow_datasets as tfds\n",
        "from datetime import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FlXYDGzfhCa",
        "colab_type": "text"
      },
      "source": [
        "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGCbDx8_i7U5",
        "colab_type": "text"
      },
      "source": [
        "Fetching BERT Model from TensorFlow Hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WfxBBG4i_jZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_layer = tfhub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDBO3BmKoMvP",
        "colab_type": "text"
      },
      "source": [
        "### Importing and Cleaning Spam Messages Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSMb7hKFzTYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train = pd.read_csv('train.csv')\n",
        "Test = pd.read_csv('test.csv')\n",
        "\n",
        "# Removing Non-Alphabet Characters\n",
        "def remove_non_alphabet(x):\n",
        "    return ' '.join([i for i in x.split() if i.isalpha() == True])\n",
        "\n",
        "# Lowering Words\n",
        "def lowerwords(text):\n",
        "\ttext = re.sub(\"[^a-zA-Z]\",\" \",text) # Excluding Numbers\n",
        "\ttext = [word.lower() for word in text.split()]\n",
        "    # joining the list of words with space separator\n",
        "\treturn \" \".join(text)\n",
        "\n",
        "\n",
        "# Removing Punctuation\n",
        "def remove_punctuation(text):\n",
        "    '''a function for removing punctuation'''\n",
        "    # replacing the punctuations with no space, \n",
        "    # which in effect deletes the punctuation marks \n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    # return the text stripped of punctuation marks\n",
        "    return text.translate(translator)\n",
        "\n",
        "\n",
        "# Removing StopWords\n",
        "def remove_stopwords(text):\n",
        "    StopWords = set(stopwords.words('english'))\n",
        "    output = ' '.join([i for i in text.split() if i not in StopWords])\n",
        "    return output\n",
        "\n",
        "\n",
        "def remove_urls(text):\n",
        "    text = re.sub(r'ttps?://\\S+|www\\.\\S+<.*?>', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "\n",
        "# Lemmatizer\n",
        "def Lemmatizing(description):\n",
        "    description = nlp.word_tokenize(description)\n",
        "    #description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n",
        "    lemma = nlp.WordNetLemmatizer()\n",
        "    description = [lemma.lemmatize(word) for word in description]\n",
        "    description = \" \".join(description)\n",
        "    \n",
        "    return description\n",
        "\n",
        "def remove_emoji(text):\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    return emoji_pattern.sub(r'', text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTfrYGxb4QBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Train['text'] = Train['text'].apply(remove_urls)\n",
        "Train['text'] = Train['text'].apply(remove_emoji)\n",
        "Train['text'] = Train['text'].apply(remove_punctuation)\n",
        "Train['text'] = Train['text'].apply(remove_non_alphabet)\n",
        "Train['text'] = Train['text'].apply(lowerwords)\n",
        "Train['text'] = Train['text'].apply(Lemmatizing)\n",
        "Train['text'] = Train['text'].apply(remove_stopwords)\n",
        "\n",
        "Test['text'] = Test['text'].apply(remove_urls)\n",
        "Test['text'] = Test['text'].apply(remove_emoji)\n",
        "Test['text'] = Test['text'].apply(remove_punctuation)\n",
        "Test['text'] = Test['text'].apply(remove_non_alphabet)\n",
        "Test['text'] = Test['text'].apply(lowerwords)\n",
        "Test['text'] = Test['text'].apply(Lemmatizing)\n",
        "Test['text'] = Test['text'].apply(remove_stopwords)\n",
        "\n",
        "\n",
        "X_Train = Train['text']\n",
        "y_Labels = Train['target']\n",
        "X_Test = Test['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctzo0PnAp5Cj",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation for BERT Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw6wU-ps4tK7",
        "colab_type": "text"
      },
      "source": [
        "#### Creating a BERT Tokenizer\n",
        "\n",
        "We will first create an object of the FullTokenizer class from the bert.bert_tokenization module. Next, we create a BERT Embedding Layer by importing the BERT Model from tfhub.KerasLayer. The trainable parameter is set to False, which means that we will not be training the BERT Embedding. In the next line, we create a BERT Vocabulary file in the form a numpy array. We then set the text to lowercase and finally we pass our vocabulary_file and to_lower_case variables to the BertTokenizer object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgyJSbBC4xJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = tfhub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
        "                            trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0baP8fYm59h-",
        "colab_type": "text"
      },
      "source": [
        "#### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzqw7zj3p9R0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_masks(tokens,max_seq_length):\n",
        "    \"\"\"\n",
        "    This Function Trims/ Pads a depending on length of token\n",
        "    \"\"\"\n",
        "    if len(tokens)>max_seq_length:\n",
        "        # Cutting Down the Excess Length\n",
        "        tokens = tokens[0:max_seq_length]\n",
        "        return [1]*len(tokens)\n",
        "    else :\n",
        "        return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_segments(tokens, max_seq_length):\n",
        "    \n",
        "    if len(tokens)>max_seq_length:\n",
        "        # Cutting Down the Excess Length\n",
        "        tokens = tokens[:max_seq_length]\n",
        "        segments = []\n",
        "        current_segment_id = 0\n",
        "        for token in tokens:\n",
        "            segments.append(current_segment_id)\n",
        "            if token == \"[SEP]\":\n",
        "                current_segment_id = 1\n",
        "        return segments\n",
        "    \n",
        "    else:\n",
        "        segments = []\n",
        "        current_segment_id = 0\n",
        "        for token in tokens:\n",
        "            segments.append(current_segment_id)\n",
        "            if token == \"[SEP]\":\n",
        "                current_segment_id = 1\n",
        "        return segments + [0] * (max_seq_length - len(tokens))\n",
        "\n",
        "\n",
        "def get_ids(tokens, tokenizer, max_seq_length):    \n",
        "    if len(tokens)>max_seq_length:\n",
        "        tokens = tokens[:max_seq_length]\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        return token_ids\n",
        "    else:\n",
        "        token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n",
        "        return input_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIktltvtE44k",
        "colab_type": "text"
      },
      "source": [
        "Creating Data for BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vqkFIdx6hY0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CreatingData(X_Train,tokenizer,max_seq_length=150):\n",
        "    \n",
        "    X_IDs = []\n",
        "    X_Masks = []\n",
        "    X_Segments = []\n",
        "\n",
        "    for i in range(X_Train.shape[0]):\n",
        "        x = X_Train[i]\n",
        "        x = tokenizer.tokenize(x)\n",
        "        x = [\"[CLS]\"] + x + [\"[SEP]\"]\n",
        "\n",
        "        X_IDs.append(get_ids(x, tokenizer, max_seq_length))\n",
        "        X_Masks.append(get_masks(x,max_seq_length))\n",
        "        X_Segments.append(get_segments(x, max_seq_length))\n",
        "\n",
        "    return np.array(X_IDs), np.array(X_Masks), np.array(X_Segments)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGeYOJBSF2c7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "dfa6be67-9533-49be-d55e-77c84b4cdf07"
      },
      "source": [
        "X_Train_IDs, X_Train_Masks, X_Train_Segments = CreatingData(X_Train,tokenizer)\n",
        "X_Test_IDs, X_Test_Masks, X_Test_Segments = CreatingData(X_Test,tokenizer)\n",
        "print (X_Train_IDs.shape)\n",
        "print (X_Test_IDs.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7613, 150)\n",
            "(3263, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zh8GEduMQHWs",
        "colab_type": "text"
      },
      "source": [
        "### Creating Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDRy3QXRbN31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Build_Model(bert_layer=bert_layer,Max_Seq_Length=150):\n",
        "    IDs = Input(shape=(Max_Seq_Length,), dtype=tf.int32)\n",
        "    Masks = Input(shape=(Max_Seq_Length,), dtype=tf.int32)\n",
        "    Segments = Input(shape=(Max_Seq_Length,), dtype=tf.int32)\n",
        "\n",
        "    Pooled_Output, Sequence_Output = bert_layer([IDs,Masks,Segments])\n",
        "\n",
        "    x = Sequence_Output[:,0,:]\n",
        "    x = Dropout(0.2)(x)\n",
        "    Outputs = Dense(1,activation=\"sigmoid\")(x)\n",
        "\n",
        "    return Model(inputs=[IDs,Masks,Segments],outputs=Outputs)\n",
        "\n",
        "Model = Build_Model()\n",
        "Model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','AUC'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jvme6yC_nNFX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "21a9872d-31d0-4812-fc51-277ab15726ff"
      },
      "source": [
        "Model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_4 (InputLayer)            [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_5 (InputLayer)            [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 150)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "keras_layer_1 (KerasLayer)      [(None, 768), (None, 109482241   input_4[0][0]                    \n",
            "                                                                 input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_strided_slice_1 (Te [(None, 768)]        0           keras_layer_1[1][1]              \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 768)          0           tf_op_layer_strided_slice_1[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1)            769         dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 769\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USi3sGG_dF5h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "eab1d0c9-8840-4586-fb69-26f072090c1e"
      },
      "source": [
        "Model.fit([X_Train_IDs, X_Train_Masks, X_Train_Segments], y_Labels, epochs=25, batch_size=128, validation_split=0.1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 155s 3s/step - loss: 0.6139 - accuracy: 0.6704 - auc: 0.7078 - val_loss: 0.5349 - val_accuracy: 0.7822 - val_auc: 0.8231\n",
            "Epoch 2/25\n",
            "54/54 [==============================] - 153s 3s/step - loss: 0.5361 - accuracy: 0.7456 - auc: 0.8000 - val_loss: 0.4947 - val_accuracy: 0.7900 - val_auc: 0.8418\n",
            "Epoch 3/25\n",
            "54/54 [==============================] - 153s 3s/step - loss: 0.5118 - accuracy: 0.7606 - auc: 0.8176 - val_loss: 0.4765 - val_accuracy: 0.7808 - val_auc: 0.8507\n",
            "Epoch 4/25\n",
            "54/54 [==============================] - 152s 3s/step - loss: 0.4981 - accuracy: 0.7723 - auc: 0.8278 - val_loss: 0.4659 - val_accuracy: 0.7835 - val_auc: 0.8568\n",
            "Epoch 5/25\n",
            "54/54 [==============================] - 153s 3s/step - loss: 0.4877 - accuracy: 0.7778 - auc: 0.8365 - val_loss: 0.4589 - val_accuracy: 0.7861 - val_auc: 0.8608\n",
            "Epoch 6/25\n",
            "54/54 [==============================] - 152s 3s/step - loss: 0.4804 - accuracy: 0.7818 - auc: 0.8406 - val_loss: 0.4570 - val_accuracy: 0.8018 - val_auc: 0.8631\n",
            "Epoch 7/25\n",
            "54/54 [==============================] - 153s 3s/step - loss: 0.4760 - accuracy: 0.7863 - auc: 0.8438 - val_loss: 0.4522 - val_accuracy: 0.7887 - val_auc: 0.8648\n",
            "Epoch 8/25\n",
            "54/54 [==============================] - 152s 3s/step - loss: 0.4701 - accuracy: 0.7859 - auc: 0.8477 - val_loss: 0.4485 - val_accuracy: 0.7940 - val_auc: 0.8662\n",
            "Epoch 9/25\n",
            "54/54 [==============================] - 152s 3s/step - loss: 0.4677 - accuracy: 0.7886 - auc: 0.8494 - val_loss: 0.4451 - val_accuracy: 0.8045 - val_auc: 0.8677\n",
            "Epoch 10/25\n",
            "54/54 [==============================] - 153s 3s/step - loss: 0.4639 - accuracy: 0.7916 - auc: 0.8524 - val_loss: 0.4436 - val_accuracy: 0.7979 - val_auc: 0.8681\n",
            "Epoch 11/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4618 - accuracy: 0.7958 - auc: 0.8535 - val_loss: 0.4420 - val_accuracy: 0.8018 - val_auc: 0.8687\n",
            "Epoch 12/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4582 - accuracy: 0.7929 - auc: 0.8561 - val_loss: 0.4416 - val_accuracy: 0.8058 - val_auc: 0.8692\n",
            "Epoch 13/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4569 - accuracy: 0.7968 - auc: 0.8566 - val_loss: 0.4397 - val_accuracy: 0.8031 - val_auc: 0.8693\n",
            "Epoch 14/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4564 - accuracy: 0.7983 - auc: 0.8568 - val_loss: 0.4387 - val_accuracy: 0.8071 - val_auc: 0.8696\n",
            "Epoch 15/25\n",
            "54/54 [==============================] - 155s 3s/step - loss: 0.4542 - accuracy: 0.7981 - auc: 0.8580 - val_loss: 0.4386 - val_accuracy: 0.8084 - val_auc: 0.8697\n",
            "Epoch 16/25\n",
            "54/54 [==============================] - 153s 3s/step - loss: 0.4541 - accuracy: 0.7983 - auc: 0.8577 - val_loss: 0.4376 - val_accuracy: 0.8031 - val_auc: 0.8706\n",
            "Epoch 17/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4523 - accuracy: 0.7989 - auc: 0.8596 - val_loss: 0.4371 - val_accuracy: 0.8031 - val_auc: 0.8708\n",
            "Epoch 18/25\n",
            "54/54 [==============================] - 152s 3s/step - loss: 0.4502 - accuracy: 0.7999 - auc: 0.8604 - val_loss: 0.4388 - val_accuracy: 0.7900 - val_auc: 0.8709\n",
            "Epoch 19/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4529 - accuracy: 0.8000 - auc: 0.8585 - val_loss: 0.4343 - val_accuracy: 0.8045 - val_auc: 0.8716\n",
            "Epoch 20/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4507 - accuracy: 0.8005 - auc: 0.8601 - val_loss: 0.4353 - val_accuracy: 0.8018 - val_auc: 0.8715\n",
            "Epoch 21/25\n",
            "54/54 [==============================] - 155s 3s/step - loss: 0.4482 - accuracy: 0.8024 - auc: 0.8616 - val_loss: 0.4351 - val_accuracy: 0.8005 - val_auc: 0.8711\n",
            "Epoch 22/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4462 - accuracy: 0.8034 - auc: 0.8633 - val_loss: 0.4342 - val_accuracy: 0.8005 - val_auc: 0.8714\n",
            "Epoch 23/25\n",
            "54/54 [==============================] - 155s 3s/step - loss: 0.4478 - accuracy: 0.7992 - auc: 0.8623 - val_loss: 0.4343 - val_accuracy: 0.8018 - val_auc: 0.8712\n",
            "Epoch 24/25\n",
            "54/54 [==============================] - 154s 3s/step - loss: 0.4484 - accuracy: 0.7986 - auc: 0.8601 - val_loss: 0.4357 - val_accuracy: 0.8058 - val_auc: 0.8715\n",
            "Epoch 25/25\n",
            "54/54 [==============================] - 155s 3s/step - loss: 0.4471 - accuracy: 0.8032 - auc: 0.8619 - val_loss: 0.4391 - val_accuracy: 0.7992 - val_auc: 0.8709\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1f509ffa20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgdBkKRdlzSg",
        "colab_type": "text"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvBkrLHkdPQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Predictions = np.array(Model.predict([X_Test_IDs, X_Test_Masks, X_Test_Segments]))\n",
        "Predictions = np.round(Predictions.flatten()).astype(int)\n",
        "\n",
        "submission = pd.read_csv('sample_submission.csv')\n",
        "submission['target'] = Predictions\n",
        "submission.to_csv('./submission.csv', index=False, header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q34b0Iji8P5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}